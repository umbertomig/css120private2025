{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# CSS 120: Environmental Data Science\n",
    "\n",
    "## Paleoclimate\n",
    "\n",
    "### Umberto Mignozzetti (UCSD)\n",
    "\n",
    "(Based on Project Pythia and ClimateMatch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# To install\n",
    "# !pip install LiPD --quiet\n",
    "# !pip install pyleoclim --quiet\n",
    "# !pip install climlab --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# System helpers\n",
    "import os\n",
    "import sys\n",
    "from io import StringIO\n",
    "import tempfile\n",
    "\n",
    "# Data analysis\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pooch\n",
    "\n",
    "# Principal Component Analysis\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Paleodata analysis\n",
    "import lipd\n",
    "import pyleoclim as pyleo\n",
    "\n",
    "# Maps\n",
    "import cartopy, cartopy.crs as ccrs, cartopy.feature as cfeature, cartopy.io.shapereader as shapereader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##  Helper functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Pooch Load\n",
    "def pooch_load(filelocation=None, filename=None, processor=None):\n",
    "    shared_location = \"~/\"\n",
    "    user_temp_cache = tempfile.gettempdir()\n",
    "\n",
    "    if os.path.exists(os.path.join(shared_location, filename)):\n",
    "        file = os.path.join(shared_location, filename)\n",
    "    else:\n",
    "        file = pooch.retrieve(\n",
    "            filelocation,\n",
    "            known_hash=None,\n",
    "            fname=os.path.join(user_temp_cache, filename),\n",
    "            processor=processor,\n",
    "        )\n",
    "\n",
    "    return file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##  Helper functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Function to convert the PAGES2K LiDP files in a pandas.DataFrame\n",
    "def lipd2df(\n",
    "    lipd_dirpath,\n",
    "    pkl_filepath=None,\n",
    "    col_str=[\n",
    "        \"paleoData_pages2kID\", \"dataSetName\", \"archiveType\", \"geo_meanElev\", \"geo_meanLat\",\n",
    "        \"geo_meanLon\", \"year\", \"yearUnits\", \"paleoData_variableName\", \"paleoData_units\",\n",
    "        \"paleoData_values\", \"paleoData_proxy\",\n",
    "    ],\n",
    "):\n",
    "    \"\"\"\n",
    "    Convert a bunch of PAGES2k LiPD files to a `pandas.DataFrame` to boost data loading.\n",
    "\n",
    "    If `pkl_filepath` isn't `None`, save the DataFrame as a pikle file.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "        lipd_dirpath: str\n",
    "          Path of the PAGES2k LiPD files\n",
    "        pkl_filepath: str or None\n",
    "          Path of the converted pickle file. Default: `None`\n",
    "        col_str: list of str\n",
    "          Name of the variables to extract from the LiPD files\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "        df: `pandas.DataFrame`\n",
    "          Converted Pandas DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    # Save the current working directory for later use, as the LiPD utility will change it in the background\n",
    "    work_dir = os.getcwd()\n",
    "    # LiPD utility requries the absolute path\n",
    "    lipd_dirpath = os.path.abspath(lipd_dirpath)\n",
    "    # Load LiPD files\n",
    "    lipds = lipd.readLipd(lipd_dirpath)\n",
    "    # Extract timeseries from the list of LiDP objects\n",
    "    ts_list = lipd.extractTs(lipds)\n",
    "    # Recover the working directory\n",
    "    os.chdir(work_dir)\n",
    "    # Create an empty pandas.DataFrame with the number of rows to be the number of the timeseries (PAGES2k records),\n",
    "    # and the columns to be the variables we'd like to extract\n",
    "    df_tmp = pd.DataFrame(index=range(len(ts_list)), columns=col_str)\n",
    "    # Loop over the timeseries and pick those for global temperature analysis\n",
    "    i = 0\n",
    "    for ts in ts_list:\n",
    "        if (\n",
    "            \"paleoData_useInGlobalTemperatureAnalysis\" in ts.keys()\n",
    "            and ts[\"paleoData_useInGlobalTemperatureAnalysis\"] == \"TRUE\"\n",
    "        ):\n",
    "            for name in col_str:\n",
    "                try:\n",
    "                    df_tmp.loc[i, name] = ts[name]\n",
    "                except:\n",
    "                    df_tmp.loc[i, name] = np.nan\n",
    "            i += 1\n",
    "    # Drop the rows with all NaNs (those not for global temperature analysis)\n",
    "    df = df_tmp.dropna(how=\"all\")\n",
    "    # Save the dataframe to a pickle file for later use\n",
    "    if pkl_filepath:\n",
    "        save_path = os.path.abspath(pkl_filepath)\n",
    "        print(f\"Saving pickle file at: {save_path}\")\n",
    "        df.to_pickle(save_path)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##  Helper functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupressOutputs(list):\n",
    "    def __enter__(self):\n",
    "        self._stdout = sys.stdout\n",
    "        sys.stdout = self._stringio = StringIO()\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, *args):\n",
    "        self.extend(self._stringio.getvalue().splitlines())\n",
    "        del self._stringio  # free up some memory\n",
    "        sys.stdout = self._stdout##  Helper functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Reconstructing Past Changes in Terrestrial Climate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reconstructing Past Changes in Terrestrial Climate\n",
    "\n",
    "Let us now look at temperature change over the past 2,000 years as recorded by proxy records from tree rings, speleothems, and lake sediments. \n",
    "\n",
    "To analyze these datasets, we will group them by archive and create time series plots to assess temperature variations.\n",
    "\n",
    "We will:\n",
    "\n",
    "- Plot temperature records based on three different terrestrial proxies\n",
    "- Assess similarities and differences between the temperature records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Loading Terrestrial Paleoclimate Records\n",
    "\n",
    "First, we need to download the data. Since it is stored as a LiPD file, we will use Pyleoclim to format and interpret the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# set the name to save the Euro2k data\n",
    "fname = \"euro2k_data\"\n",
    "\n",
    "# download the data\n",
    "lipd_file_path = pooch.retrieve(\n",
    "    url=\"https://osf.io/7ezp3/download/\",\n",
    "    known_hash=None,\n",
    "    path=\"./\",\n",
    "    fname=fname,\n",
    "    processor=pooch.Unzip(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Loading Terrestrial Paleoclimate Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "with SupressOutputs():\n",
    "    d_euro = pyleo.Lipd(os.path.join(\".\", f\"{fname}.unzip\", \"Euro2k\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Temperature Reconstructions\n",
    "\n",
    "We can filter all of the data so that we only keep reconstructions of temperature from terrestrial archives (e.g. tree rings, speleothems and lake sediments). \n",
    "\n",
    "This is accomplished with the function below. \n",
    "\n",
    "The [`Lipd.to_tso`](https://pyleoclim-util.readthedocs.io/en/latest/core/api.html#pyleoclim.core.lipd.Lipd.to_tso) method is used to obtain a list of dictionaries that can be iterated upon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "def filter_data(dataset, archive_type, variable_name):\n",
    "    \"\"\"\n",
    "    Return a MultipleSeries object with the variable \n",
    "    record (variable_name) for a given archive_type and coordinates.\n",
    "    \"\"\"\n",
    "    # Create a list of dictionary that can be iterated upon using Lipd.to_tso method\n",
    "    ts_list = dataset.to_tso()\n",
    "    # Append the correct indices for a given value of archive_type and variable_name\n",
    "    indices = []\n",
    "    lat = []\n",
    "    lon = []\n",
    "    for idx, item in enumerate(ts_list):\n",
    "        # Check that it is available to avoid errors on the loop\n",
    "        if \"archiveType\" in item.keys():\n",
    "            # If it's a archive_type, then proceed to the next step\n",
    "            if item[\"archiveType\"] == archive_type:\n",
    "                if item[\"paleoData_variableName\"] == variable_name:\n",
    "                    indices.append(idx)\n",
    "    print(indices)\n",
    "    # Create a list of LipdSeries for the given indices\n",
    "    ts_list_archive_type = []\n",
    "    for indice in indices:\n",
    "        ts_list_archive_type.append(pyleo.LipdSeries(ts_list[indice]))\n",
    "\n",
    "        # save lat and lons of proxies\n",
    "        lat.append(ts_list[indice][\"geo_meanLat\"])\n",
    "        lon.append(ts_list[indice][\"geo_meanLon\"])\n",
    "\n",
    "    return pyleo.MultipleSeries(ts_list_archive_type), lat, lon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Temperature Reconstructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "ts_list = d_euro.to_tso()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Temperature Reconstructions\n",
    "\n",
    "Dictionaries are native to Python and can be explored as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# look at available entries for just one time-series\n",
    "ts_list[0].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Temperature Reconstructions\n",
    "\n",
    "Dictionaries are native to Python and can be explored as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# print relevant information for all entries\n",
    "for idx, item in enumerate(ts_list):\n",
    "    print(str(idx) + \": \" + item[\"dataSetName\"] +\n",
    "          \": \" + item[\"paleoData_variableName\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Temperature Reconstructions\n",
    "\n",
    "Now let's use our pre-defined function to create a new list that only has temperature reconstructions based on proxies from **lake sediments**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {},
    "executionInfo": {
     "elapsed": 163,
     "status": "ok",
     "timestamp": 1680674812384,
     "user": {
      "displayName": "Brodie Pearson",
      "userId": "05269028596972519847"
     },
     "user_tz": 420
    }
   },
   "outputs": [],
   "source": [
    "ms_euro_lake, euro_lake_lat, euro_lake_lon = filter_data(\n",
    "    d_euro, \"lake sediment\", \"temperature\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Temperature Reconstructions\n",
    "\n",
    "And a new list that only has temperature reconstructions based on proxies from **tree rings**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {},
    "executionInfo": {
     "elapsed": 187,
     "status": "ok",
     "timestamp": 1680674816989,
     "user": {
      "displayName": "Brodie Pearson",
      "userId": "05269028596972519847"
     },
     "user_tz": 420
    }
   },
   "outputs": [],
   "source": [
    "ms_euro_tree, euro_tree_lat, euro_tree_lon = filter_data(\n",
    "    d_euro, \"tree\", \"temperature\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Temperature Reconstructions\n",
    "\n",
    "And a new list that only has temperature information based on proxies from **speleothems**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {},
    "executionInfo": {
     "elapsed": 200,
     "status": "ok",
     "timestamp": 1680674825774,
     "user": {
      "displayName": "Brodie Pearson",
      "userId": "05269028596972519847"
     },
     "user_tz": 420
    }
   },
   "outputs": [],
   "source": [
    "ms_euro_spel, euro_spel_lat, euro_spel_lon = filter_data(\n",
    "    d_euro, \"speleothem\", \"d18O\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Temperature Reconstructions\n",
    "\n",
    "Since we are going to compare temperature datasets based on different terrestrial climate archives, the quantitative values of the measurements in each record will differ. \n",
    "\n",
    "Therefore, to more easily and accurately compare temperature between the records, it's helpful to standardize the data. \n",
    "\n",
    "The `.standardize()` function removes the estimated mean of the time series and divides by its estimated standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# standardize the data\n",
    "spel_stnd = ms_euro_spel.standardize()\n",
    "lake_stnd = ms_euro_lake.standardize()\n",
    "tree_stnd = ms_euro_tree.standardize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Temperature Reconstructions\n",
    "\n",
    "Now we can use Pyleoclim functions to create three stacked plots of this data with lake sediment records on top, tree ring reconstructions in the middle and speleothem records on the bottom.\n",
    "\n",
    "Note that the colors used for the time series in each plot are the default colors generated by the function, so the corresponding colors in each of the three plots are not relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {},
    "executionInfo": {
     "elapsed": 4905,
     "status": "ok",
     "timestamp": 1680674844494,
     "user": {
      "displayName": "Brodie Pearson",
      "userId": "05269028596972519847"
     },
     "user_tz": 420
    }
   },
   "outputs": [],
   "source": [
    "# note the x axis is years before present, so read from left to right moving back in time\n",
    "\n",
    "ax = lake_stnd.stackplot(\n",
    "    label_x_loc=1.7,\n",
    "    xlim=[0, 2000],\n",
    "    v_shift_factor=1,\n",
    "    figsize=[9, 5],\n",
    "    time_unit=\"yrs BP\",\n",
    ")\n",
    "ax[0].suptitle(\"Lake Cores\", y=1.2)\n",
    "\n",
    "ax = tree_stnd.stackplot(\n",
    "    label_x_loc=1.7,\n",
    "    xlim=[0, 2000],\n",
    "    v_shift_factor=1,\n",
    "    figsize=[9, 5],\n",
    "    time_unit=\"yrs BP\",\n",
    ")\n",
    "ax[0].suptitle(\"Tree Rings\", y=1.2)\n",
    "\n",
    "# recall d18O is a proxy for SST, and that more positive d18O means colder SST\n",
    "ax = spel_stnd.stackplot(\n",
    "    label_x_loc=1.7,\n",
    "    xlim=[0, 2000],\n",
    "    v_shift_factor=1,\n",
    "    figsize=[9, 5],\n",
    "    time_unit=\"yrs BP\",\n",
    ")\n",
    "ax[0].suptitle(\"Speleothems\", y=1.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Reconstructing Past Changes in Atmospheric Climate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Reconstructing Past Changes in Atmospheric Climate\n",
    "\n",
    "To understand past atmospheric climate changes, we’ll analyze δD and atmospheric CO<sub>2</sub> data from the EPICA Dome C ice core.\n",
    "\n",
    "δD and δ<sup>18</sup>O measurements on ice cores record past changes in temperature, and that measurements of CO<sub>2</sub> trapped in ice cores can be used to reconstruction past changes in Earth's atmospheric composition.\n",
    "\n",
    "We will:\n",
    "\n",
    "- Plot δD and CO<sub>2</sub> records from the EPICA Dome C ice core\n",
    "- Assess changes in temperature and atmospheric greenhouse gas concentration over the past 800,000 years "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exploring past variations in atmospheric CO$_2$\n",
    "\n",
    "Paleoclimatologists can reconstruct past changes in atmospheric composition by measuring gases trapped in layers of ice from ice cores retrieved from polar regions and high elevation mountain glaciers. \n",
    "\n",
    "We'll specifically be focusing on paleoclimate records produced from the [EPICA Dome C](https://en.wikipedia.org/wiki/Dome_C) ice core from Antarctica.\n",
    "\n",
    "![](https://media.springernature.com/full/springer-static/image/art%3A10.1038%2Fncomms8850/MediaObjects/41467_2015_Article_BFncomms8850_Fig1_HTML.jpg?as=webp)\n",
    "\n",
    "Credit: [Conway et al 2015, *Nature Communications*](https://www.nature.com/articles/ncomms8850)\n",
    "\n",
    "Let's start by downloading the data for the composite CO<sub>2</sub> record for EPICA Dome C in Antarctica:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exploring past variations in atmospheric CO$_2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {},
    "executionInfo": {
     "elapsed": 853,
     "status": "ok",
     "timestamp": 1680729021239,
     "user": {
      "displayName": "Brodie Pearson",
      "userId": "05269028596972519847"
     },
     "user_tz": 420
    }
   },
   "outputs": [],
   "source": [
    "filename_antarctica2015 = \"antarctica2015co2composite.txt\"\n",
    "url_antarctica2015 = \"https://www.ncei.noaa.gov/pub/data/paleo/icecore/antarctica/antarctica2015co2composite.txt\"\n",
    "\n",
    "data_path = pooch_load(\n",
    "    filelocation=url_antarctica2015, filename=filename_antarctica2015\n",
    ")  # open the file\n",
    "\n",
    "co2df = pd.read_csv(data_path, skiprows=137, sep=\"\\t\")\n",
    "\n",
    "co2df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exploring past variations in atmospheric CO$_2$\n",
    "\n",
    "Store this data as a `Series` in Pyleoclim:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "ts_co2 = pyleo.Series(\n",
    "    time=co2df[\"age_gas_calBP\"] / 1000,\n",
    "    value=co2df[\"co2_ppm\"],\n",
    "    time_name=\"Age\",\n",
    "    time_unit=\"kyr BP\",\n",
    "    value_name=r\"$CO_2$\",\n",
    "    value_unit=\"ppm\",\n",
    "    label=\"EPICA Dome C CO2\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exploring past variations in atmospheric CO$_2$\n",
    "\n",
    "We can now plot age vs. CO<sub>2</sub> from EPICA Dome C:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {},
    "executionInfo": {
     "elapsed": 800,
     "status": "ok",
     "timestamp": 1680729048261,
     "user": {
      "displayName": "Brodie Pearson",
      "userId": "05269028596972519847"
     },
     "user_tz": 420
    }
   },
   "outputs": [],
   "source": [
    "ts_co2.plot(color=\"C1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exploring past variations in atmospheric CO$_2$\n",
    "\n",
    "Notice that the x-axis is plotted with present-day (0 kyr) on the left and the past (800 kyr) on the right. This is a common practice when plotting paleoclimate time series data.\n",
    "\n",
    "These changes in CO<sub>2</sub> are tracking glacial-interglacial cycles (Ice Ages) over the past 800,000 years. \n",
    "\n",
    "Recall that these Ice Ages occur as a result of changes in the orbital cycles of Earth: eccentricity (100,000 year cycle), obliquity (40,000 year cycle) and precession (21,000 year cycle).\n",
    "\n",
    "Can you observe them in the graph above?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exploring the relationship between δD and atmospheric CO<sub>2</sub>\n",
    "\n",
    "To investigate the relationship between glacial cycles, atmospheric CO<sub>2</sub> and temperature, we can compare CO<sub>2</sub> to a record of hydrogen isotopic values (δD) of ice cores, which is a proxy for temperature in this case. \n",
    "\n",
    "***When interpreting isotopic measurements of ice cores, a more depleted δD value indicates cooler temperatures, and a more enriched δD value indicates warmer temperatures.***\n",
    "\n",
    "This is the opposite relationship we have looked at previously with &delta;<sup>18</sup>O, not because we are looking at a different isotope, but because we are not looking at the isotopic composition of ice rather than the isotopic composition of the ocean.\n",
    "\n",
    "Let's download the EPICA Dome C δD data, store it as a `Series`, and plot the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {},
    "executionInfo": {
     "elapsed": 1120,
     "status": "ok",
     "timestamp": 1680729854978,
     "user": {
      "displayName": "Brodie Pearson",
      "userId": "05269028596972519847"
     },
     "user_tz": 420
    }
   },
   "outputs": [],
   "source": [
    "filename_edc3deuttemp2007 = \"edc3deuttemp2007.txt\"\n",
    "url_edc3deuttemp2007 = \"https://www.ncei.noaa.gov/pub/data/paleo/icecore/antarctica/epica_domec/edc3deuttemp2007.txt\"\n",
    "data_path = pooch_load(\n",
    "    filelocation=url_edc3deuttemp2007, filename=filename_edc3deuttemp2007\n",
    ")  # open the file\n",
    "\n",
    "dDdf = pd.read_csv(data_path, skiprows=91, encoding=\"unicode_escape\", sep=\"\\s+\")\n",
    "dDdf.dropna(inplace=True)\n",
    "dDdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exploring the relationship between δD and atmospheric CO<sub>2</sub>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "dDts = pyleo.Series(\n",
    "    time=dDdf[\"Age\"] / 1000,\n",
    "    value=dDdf[\"Deuterium\"],\n",
    "    time_name=\"Age\",\n",
    "    time_unit=\"kyr BP\",\n",
    "    value_name=r\"$\\delta D$\",\n",
    "    value_unit=\"\\u2030\",\n",
    "    label=r\"EPICA Dome C $\\delta D$\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exploring the relationship between δD and atmospheric CO<sub>2</sub>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {},
    "executionInfo": {
     "elapsed": 588,
     "status": "ok",
     "timestamp": 1680729931069,
     "user": {
      "displayName": "Brodie Pearson",
      "userId": "05269028596972519847"
     },
     "user_tz": 420
    }
   },
   "outputs": [],
   "source": [
    "dDts.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exploring the relationship between δD and atmospheric CO<sub>2</sub>\n",
    "\n",
    "When we observe the δD data, we see very similar patterns as in the atmospheric CO<sub>2</sub> data. \n",
    "\n",
    "To more easily compare the two records, we can plot the two series side by side by putting them into a `MultipleSeries` object. \n",
    "\n",
    "Since the δD and CO<sub>2</sub> values have different units, we can first standardize the series and then plot the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {},
    "executionInfo": {
     "elapsed": 690,
     "status": "ok",
     "timestamp": 1680730119909,
     "user": {
      "displayName": "Brodie Pearson",
      "userId": "05269028596972519847"
     },
     "user_tz": 420
    }
   },
   "outputs": [],
   "source": [
    "# combine series\n",
    "ms = pyleo.MultipleSeries([dDts, ts_co2])\n",
    "\n",
    "# standarize series and plot\n",
    "ms.standardize().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exploring the relationship between δD and atmospheric CO<sub>2</sub>\n",
    "\n",
    "Now we can more easily compare the timing and magnitude of changes in CO<sub>2</sub> and δD at EPICA Dome C over the past 800,000 years. \n",
    "\n",
    "During glacial periods, δD was more depleted (cooler temperatures) and atmospheric CO<sub>2</sub>  was lower. \n",
    "\n",
    "During interglacial periods, δD was more enriched (warmer temperatures) and atmospheric CO<sub>2</sub>  was higher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Paleoclimate Data Analysis Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Paleoclimate Data Analysis Tools\n",
    "\n",
    "A common issue in paleoclimate is the presence of uneven time spacing between consecutive observations. \n",
    "\n",
    "`Pyleoclim` includes several methods that can deal with uneven sampling effectively, but there are certain applications and analyses for which it's ncessary to place the records on a uniform time axis. Let us study a few ways to do this with `Pyleoclim`. \n",
    "\n",
    "Additionally, we will explore another useful paleoclimate data analysis tool, Principal Component Analysis (PCA), which allows us to identify a common signal between various paleoclimate reconstructions. \n",
    "\n",
    "We should be able to perform the following data analysis techniques on proxy-based climate reconstructions:\n",
    "\n",
    "*   Interpolation\n",
    "*   Binning \n",
    "*   Principal component analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Load the sample dataset for analysis\n",
    "\n",
    "The dataset we'll be using is a record of hydrogen isotopes of leaf waxes (δD<sub>wax</sub>) from Lake Tanganyika in East Africa [(Tierney et al., 2008)](https://www.science.org/doi/10.1126/science.1160485?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub%20%200pubmed). \n",
    "\n",
    "Recall that δD<sub>wax</sub> is a proxy that is typically thought to record changes in the amount of precipitation in the tropics via the amount effect. \n",
    "\n",
    "Let's first read the data from a .csv file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Load the sample dataset for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "filename_tang = \"tanganyika_dD.csv\"\n",
    "url_tang = \"https://osf.io/sujvp/download/\"\n",
    "tang_dD = pd.read_csv(pooch_load(filelocation=url_tang, filename=filename_tang))\n",
    "tang_dD.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Load the sample dataset for analysis\n",
    "\n",
    "We can now create a `Series` in Pyleoclim and assign names to different variables so that we can easily plot the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {},
    "executionInfo": {
     "elapsed": 736,
     "status": "ok",
     "timestamp": 1681484071869,
     "user": {
      "displayName": "Sloane Garelick",
      "userId": "04706287370408131987"
     },
     "user_tz": 240
    }
   },
   "outputs": [],
   "source": [
    "ts_tang = pyleo.Series(\n",
    "    time=tang_dD[\"Age\"],\n",
    "    value=tang_dD[\"dD_IVonly\"],\n",
    "    time_name=\"Age\",\n",
    "    time_unit=\"yr BP\",\n",
    "    value_name=\"dDwax\",\n",
    "    value_unit=\"per mille\",\n",
    "    label=\"Lake Tanganyika dDprecip\",\n",
    ")\n",
    "\n",
    "ts_tang.plot(color=\"C1\", invert_yaxis=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Load the sample dataset for analysis\n",
    "\n",
    "You may notice that the y-axis is inverted. \n",
    "\n",
    "When we're plotting δD data, we typically invert the y-axis because more negative (\"depleted\") values suggest increased rainfall, whereas more positive (\"enriched\") values suggest decreased rainfall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Uniform Time-Sampling of the Data\n",
    "\n",
    "There are a number of different reasons we might want to assign new values to our data. \n",
    "\n",
    "For example, if the data is not evenly spaced, we might need to resample in order to use a sepcific data analysis technique or to more easily compare to other data of a different sampling resolution. \n",
    "\n",
    "First, let's check whether our data is already evenly spaced using the `.is_evenly_spaced()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {},
    "executionInfo": {
     "elapsed": 138,
     "status": "ok",
     "timestamp": 1681484075182,
     "user": {
      "displayName": "Sloane Garelick",
      "userId": "04706287370408131987"
     },
     "user_tz": 240
    }
   },
   "outputs": [],
   "source": [
    "ts_tang.is_evenly_spaced()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Load the sample dataset for analysis\n",
    "\n",
    "Our data is not evenly spaced. \n",
    "\n",
    "There are a few different methods available in `pyleoclim` to place the data on a uniform axis.\n",
    "\n",
    "We will interpolating and binning the data now. \n",
    "\n",
    "In general, these methods use the available data near a chosen time to estimate what the value was at that time, but each method differs in which nearby data points it uses and how it uses them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Interpolation\n",
    "\n",
    "Interpolation projects the data onto an evenly spaced time axis with a distance between points (step size) of our choosing. \n",
    "\n",
    "There are a variety of different methods by which the data can be interpolated, these being: \n",
    "\n",
    "- `linear`\n",
    "- `nearest`\n",
    "- `zero`\n",
    "- `slinear`\n",
    "- `quadratic`\n",
    "- `cubic`\n",
    "- `previous`\n",
    "- `next`. \n",
    "\n",
    "More on these and their associated key word arguments can be found in the [documentation](https://pyleoclim-util.readthedocs.io/en/latest/core/api.html#pyleoclim.core.series.Series.interp). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Interpolation\n",
    "\n",
    "By default, the function `.interp()` implements linear interpolation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "tang_linear = ts_tang.interp()  # default method = 'linear'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {},
    "executionInfo": {
     "elapsed": 167,
     "status": "ok",
     "timestamp": 1681484080381,
     "user": {
      "displayName": "Sloane Garelick",
      "userId": "04706287370408131987"
     },
     "user_tz": 240
    }
   },
   "outputs": [],
   "source": [
    "# check whether or not the series is now evenly spaced\n",
    "tang_linear.is_evenly_spaced()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Interpolation\n",
    "\n",
    "Now that we've interpolated our data, let's compare the original dataset to the linearly interpolated dataset we just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {},
    "executionInfo": {
     "elapsed": 443,
     "status": "ok",
     "timestamp": 1681484174043,
     "user": {
      "displayName": "Sloane Garelick",
      "userId": "04706287370408131987"
     },
     "user_tz": 240
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()  # assign a new plot axis\n",
    "ts_tang.plot(ax=ax, label=\"Original\", invert_yaxis=True)\n",
    "tang_linear.plot(ax=ax, label=\"Linear\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Interpolation\n",
    "\n",
    "Notice there are only some minor differences between the original and linearly interpolated data.\n",
    "\n",
    "You can print the data in the original and interpolated time series to see the difference in the ages between the two datasets. The interpolated dataset is now evenly spaced with a δD value every ~290 years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "ts_tang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "tang_linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Interpolation\n",
    "\n",
    "Let's compare a few of the different interpolation methods (e.g., quadratic, next, zero) with one another just to see how they are similar and different:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()  # assign a new plot axis\n",
    "ts_tang.plot(ax=ax, label=\"original\", invert_yaxis=True)\n",
    "for method in [\"linear\", \"quadratic\", \"next\", \"zero\"]:\n",
    "    ts_tang.interp(method=method).plot(\n",
    "        ax=ax, label=method, alpha=0.9\n",
    "    )  # plot all the method we want"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "The methods can produce slightly different results, but mostly reproduce the same overall trend. In this case, the quadractic method may be less appropriate than the other methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Binning\n",
    "\n",
    "Another option for resampling our data onto a uniform time axis is binning.\n",
    "\n",
    "Binning is when a set of time intervals is defined and data is grouped or binned with other data in the same interval, then all those points in a \"bin\" are averaged to get a data value for that bin. \n",
    "\n",
    "The defaults for binning pick a bin size at the coarsest time spacing present in the dataset and average data over a uniform sequence of such intervals. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "tang_bin = (\n",
    "    ts_tang.bin()\n",
    ")  # default settings pick the coarsest time spacing in the data as the binning period"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {},
    "executionInfo": {
     "elapsed": 457,
     "status": "ok",
     "timestamp": 1681484224785,
     "user": {
      "displayName": "Sloane Garelick",
      "userId": "04706287370408131987"
     },
     "user_tz": 240
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()  # assign a new plot axis\n",
    "ts_tang.plot(ax=ax, label=\"Original\", invert_yaxis=True)\n",
    "tang_bin.plot(ax=ax, label=\"Binned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Again, notice that although there are some minor differences between the original and binned data, the records still capture the same overall trend."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Principal Component Analysis (PCA)\n",
    "\n",
    "Large datasets, such as global climate datasets, are often difficult to interpret due to their multiple dimensions. \n",
    "\n",
    "Although tools such as Xarray help us to organize large, multidimensional climate datasets, it can still sometimes be difficult to interpret certain aspects of such data. \n",
    "\n",
    "Principal Component Analysis (PCA) is a tool for reducing the dimensionality of such datasets and increasing interpretability with minimal modification or loss of data. \n",
    "\n",
    "In other words, PCA allows us to reduce the number of variables of a dataset, while preserving as much information as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Principal Component Analysis (PCA)\n",
    "\n",
    "The first step in PCA is to calculate a matrix that summarizes how the variables in a dataset all relate to one another.\n",
    "\n",
    "This matrix is then broken down into new uncorrelated variables that are linear combinations or mixtures of the initial variables.\n",
    "\n",
    "These new variables are the **principal components**.\n",
    "\n",
    "The initial dimensions of the dataset determines the number of principal components calculated. \n",
    "\n",
    "Most of the information within the initial variables is compressed into the first components. \n",
    "\n",
    "Additional details about PCA and the calculations involved can be found [here](https://builtin.com/data-science/step-step-explanation-principal-component-analysis)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Principal Component Analysis (PCA)\n",
    "\n",
    "Applied to paleoclimate, PCA can reduce the dimensionality of large paleoclimate datasets with multiple variables and can help us identify a common signal between various paleoclimate reconstructions.\n",
    "\n",
    "An example of a study that applies PCA to paleoclimate is [Otto-Bliesner et al., 2014](https://www.science.org/doi/full/10.1126/science.1259531).\n",
    "\n",
    "This study applies PCA to rainfall reconstructions from models and proxies from throughout Africa to determine common climate signals in these reconstructions.\n",
    "\n",
    "Let us calculate the PCA of four δD paleoclimate records from Africa to assess common climate signals in the four records."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Principal Component Analysis (PCA)\n",
    "\n",
    "So far, we've been looking at δD data from Lake Tanganyika in tropical East Africa. \n",
    "\n",
    "Let's compare this δD record to other existing δD records from lake and marine sediment cores in tropical Africa from the Gulf of Aden [(Tierney and deMenocal, 2017)](https://doi.org/10.1126/science.1240411), Lake Bosumtwi [(Shanahan et al., 2015)](https://doi.org/10.1038/ngeo2329), and the West African Margin [(Tierney et al., 2017)](https://doi.org/10.1126/sciadv.1601503).\n",
    "\n",
    "First, let's load these datasets:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Principal Component Analysis (PCA)\n",
    "\n",
    "First, let's load these datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {},
    "executionInfo": {
     "elapsed": 484,
     "status": "ok",
     "timestamp": 1681484228855,
     "user": {
      "displayName": "Sloane Garelick",
      "userId": "04706287370408131987"
     },
     "user_tz": 240
    }
   },
   "outputs": [],
   "source": [
    "# Gulf of Aden\n",
    "filename_aden = \"aden_dD.csv\"\n",
    "url_aden = \"https://osf.io/gm2v9/download/\"\n",
    "aden_dD = pd.read_csv(pooch_load(filelocation=url_aden, filename=filename_aden))\n",
    "aden_dD.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Principal Component Analysis (PCA)\n",
    "\n",
    "First, let's load these datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {},
    "executionInfo": {
     "elapsed": 370,
     "status": "ok",
     "timestamp": 1681484235076,
     "user": {
      "displayName": "Sloane Garelick",
      "userId": "04706287370408131987"
     },
     "user_tz": 240
    }
   },
   "outputs": [],
   "source": [
    "# Lake Bosumtwi\n",
    "filename_Bosumtwi = \"bosumtwi_dD.csv\"\n",
    "url_Bosumtwi = \"https://osf.io/mr7d9/download/\"\n",
    "bosumtwi_dD = pd.read_csv(\n",
    "    pooch_load(filelocation=url_Bosumtwi, filename=filename_Bosumtwi)\n",
    ")\n",
    "bosumtwi_dD.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Principal Component Analysis (PCA)\n",
    "\n",
    "First, let's load these datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {},
    "executionInfo": {
     "elapsed": 354,
     "status": "ok",
     "timestamp": 1681484237400,
     "user": {
      "displayName": "Sloane Garelick",
      "userId": "04706287370408131987"
     },
     "user_tz": 240
    }
   },
   "outputs": [],
   "source": [
    "# GC27 (West African Margin)\n",
    "filename_GC27 = \"gc27_dD.csv\"\n",
    "url_GC27 = \"https://osf.io/k6e3a/download/\"\n",
    "gc27_dD = pd.read_csv(pooch_load(filelocation=url_GC27, filename=filename_GC27))\n",
    "gc27_dD.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Principal Component Analysis (PCA)\n",
    "\n",
    "Next, let's convert each dataset into a `Series` in Pyleoclim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "ts_tanganyika = pyleo.Series(\n",
    "    time=tang_dD[\"Age\"],\n",
    "    value=tang_dD[\"dD_IVonly\"],\n",
    "    time_name=\"Age\",\n",
    "    time_unit=\"yr BP\",\n",
    "    value_name=\"dDwax\",\n",
    "    label=\"Lake Tanganyika\",\n",
    ")\n",
    "ts_aden = pyleo.Series(\n",
    "    time=aden_dD[\"age_calBP\"],\n",
    "    value=aden_dD[\"dDwaxIVcorr\"],\n",
    "    time_name=\"Age\",\n",
    "    time_unit=\"yr BP\",\n",
    "    value_name=\"dDwax\",\n",
    "    label=\"Gulf of Aden\",\n",
    ")\n",
    "ts_bosumtwi = pyleo.Series(\n",
    "    time=bosumtwi_dD[\"age_calBP\"],\n",
    "    value=bosumtwi_dD[\"d2HleafwaxC31ivc\"],\n",
    "    time_name=\"Age\",\n",
    "    time_unit=\"yr BP\",\n",
    "    value_name=\"dDwax\",\n",
    "    label=\"Lake Bosumtwi\",\n",
    ")\n",
    "ts_gc27 = pyleo.Series(\n",
    "    time=gc27_dD[\"age_BP\"],\n",
    "    value=gc27_dD[\"dDwax_iv\"],\n",
    "    time_name=\"Age\",\n",
    "    time_unit=\"yr BP\",\n",
    "    value_name=\"dDwax\",\n",
    "    label=\"GC27\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Principal Component Analysis (PCA)\n",
    "\n",
    "Now let's set up a `MultipleSeries` using Pyleoclim with all four δD datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "ts_list = [ts_tanganyika, ts_aden, ts_bosumtwi, ts_gc27]\n",
    "ms_africa = pyleo.MultipleSeries(ts_list, label=\"African dDwax\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Principal Component Analysis (PCA)\n",
    "\n",
    "We can now create a stackplot with all four δD records:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {},
    "executionInfo": {
     "elapsed": 756,
     "status": "ok",
     "timestamp": 1681484266500,
     "user": {
      "displayName": "Sloane Garelick",
      "userId": "04706287370408131987"
     },
     "user_tz": 240
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = ms_africa.stackplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Principal Component Analysis (PCA)\n",
    "\n",
    "By creating a stackplot, we can visually compare between the datasets. \n",
    "\n",
    "However, the four δD records aren't the same resolution and don't span the same time interval.\n",
    "\n",
    "To better compare the records and assess a common trend, we can use PCA.\n",
    "\n",
    "First, we can use [`.common_time()`] to place the records on a shared time axis with a common sampling frequency. \n",
    "\n",
    "This function takes the argument `method`, which can be either `bin`, `interp`, and `gdkernel`. \n",
    "\n",
    "The binning and interpolation methods are what we just covered in the previous section. \n",
    "\n",
    "Let's set the time step to 500 years, the method to `interp`, and standarize the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {},
    "executionInfo": {
     "elapsed": 1048,
     "status": "ok",
     "timestamp": 1681484350993,
     "user": {
      "displayName": "Sloane Garelick",
      "userId": "04706287370408131987"
     },
     "user_tz": 240
    }
   },
   "outputs": [],
   "source": [
    "africa_ct = ms_africa.common_time(method=\"interp\", step=0.5).standardize()\n",
    "fig, ax = africa_ct.stackplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Principal Component Analysis (PCA)\n",
    "\n",
    "We now have standardized δD records that are the same sampling resolution and span the same time interval. Note this meant trimming the longer time series down to around 20,000 years in length.\n",
    "\n",
    "We can now apply PCA which will allow us to quantitatively identify a common signal between the four δD paleoclimate reconstructions through the [`.pca`](https://pyleoclim-util.readthedocs.io/en/latest/core/api.html#pyleoclim.core.multipleseries.MultipleSeries.pca) method. \n",
    "\n",
    "The `pyleoclim` package has its own pca method ( e.g. `africa_ct.pca()`), but it is computationally intensive. For our purposes we will be using the PCA method from `sklearn`.\n",
    "\n",
    "Please refer to this [example](https://vitalflux.com/pca-explained-variance-concept-python-example/) for further reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "africa_PCA =pca.fit(africa_ct.to_pandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Principal Component Analysis (PCA)\n",
    "\n",
    "The result is an object containing multiple outputs. The two outputs we'll look at is the percentage of variance accounted for by each mode as well as the principal components themselves. \n",
    "\n",
    "First, let's print the percentage of variance accounted for by each mode, noting that the first principle component is first in the list, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {},
    "executionInfo": {
     "elapsed": 224,
     "status": "ok",
     "timestamp": 1681484449770,
     "user": {
      "displayName": "Sloane Garelick",
      "userId": "04706287370408131987"
     },
     "user_tz": 240
    }
   },
   "outputs": [],
   "source": [
    "print(africa_PCA.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Principal Component Analysis (PCA)\n",
    "\n",
    "This means that most of the variance in the four paleoclimate records is explained by the first principal component (the first value displayed). The number of datasets in the PCA constrains the number of principal components that can be defined, which is why we only have four components in this example.\n",
    "\n",
    "We can now look at the principal component of the first mode of variance. Let's create a new series for the first principle component and plot it against the original datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "africa_pc1=africa_PCA.transform(africa_ct.to_pandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "africa_mode1 = pyleo.Series(\n",
    "    time=africa_ct.series_list[0].time,\n",
    "    value=africa_pc1[:,0],\n",
    "    label=r'$PC_1$',\n",
    "    value_name='PC1',\n",
    "    time_name ='age',\n",
    "    time_unit = 'yr BP'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {},
    "executionInfo": {
     "elapsed": 1026,
     "status": "ok",
     "timestamp": 1681484506972,
     "user": {
      "displayName": "Sloane Garelick",
      "userId": "04706287370408131987"
     },
     "user_tz": 240
    }
   },
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "ax1.set_ylabel(\"dDwax\")\n",
    "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "ax2.set_ylabel(\"PC1\")  # we already handled the x-label with ax1\n",
    "\n",
    "# plt.plot(mode1.time,pc1_scaled)\n",
    "africa_mode1.plot(color=\"black\", ax=ax2, invert_yaxis=True)\n",
    "africa_ct.plot(ax=ax1, linewidth=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Questions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## See you in the next class!"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "W1D4_Tutorial1",
   "provenance": [
    {
     "file_id": "1lHuVrVtAW4fQzc0dFdlZuwY0i71KWw_t",
     "timestamp": 1677637469824
    }
   ],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

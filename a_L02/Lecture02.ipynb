{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4bf78db",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# CSS 120 - Environmental Data Science\n",
    "\n",
    "## Lecture 02 -- Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad928f5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The Evolution of Data Analysis\n",
    "\n",
    "- **1970s Data Analysis**: Focused on statistics in mathematics or statistics departments.\n",
    "\n",
    "- **Today's Trend**: Machine learning (ML) in computer science departments is prevalent.\n",
    "\n",
    "- **Key Question**: Is ML just a rebranded form of statistics?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd728f9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Origins of ML and Statistics\n",
    "\n",
    "- **Statistics**: Dates back to 1749, rooted in government data collection.\n",
    "\n",
    "- **Probability's Beginnings**: Started in 1654 with Pascal and Fermat.\n",
    "\n",
    "- **Growth of Statistics**: Expanded by Pierre de Laplace in 1812 for various applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2875c4c2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Birth of Computer Science and AI\n",
    "\n",
    "- **Post-WWII Developments**: Emergence of digital programmable computers.\n",
    "\n",
    "- **AI's Inception**: Coined by John McCarthy in 1956 at the Dartmouth Conference.\n",
    "\n",
    "- **ML as AI Branch**: Concept of learning machines introduced by Turing in 1950."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90d0c0e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Origins of ML and Statistics\n",
    "\n",
    "- **Statistics**: Dates back to 1749, rooted in government data collection.\n",
    "\n",
    "- **Probability's Beginnings**: Started in 1654 with Pascal and Fermat.\n",
    "\n",
    "- **Growth of Statistics**: Expanded by Pierre de Laplace in 1812 for various applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0754f49d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The Rise and Fall of AI\n",
    "\n",
    "- **AI's Early Overpromises**: Led to two major 'AI winters' due to unrealistic expectations.\n",
    "\n",
    "- **Lighthill's Critique**: 1973 report highlighting disappointments in AI progress.\n",
    "\n",
    "- **Shift in Terminology**: Use of terms like ML, data mining to avoid AI stigma."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9170879f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ML in the Internet Era\n",
    "\n",
    "- **Data Abundance**: Boosted by the rise of the Internet in the 1990s.\n",
    "\n",
    "- **Multilayer Perceptron**: A milestone in ML development in 1986.\n",
    "\n",
    "- **Data Science Emergence**: Term introduced in late 1990s by statisticians."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e14a1c4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ML vs. Statistics in Data Science\n",
    "\n",
    "- **Distinct Cultures**: Despite overlapping, ML and statistics have different approaches.\n",
    "\n",
    "- **ML's Growth**: Fostered in environments supporting heuristic research.\n",
    "\n",
    "- **ML's 'Black Box' Nature**: Contrasts with statistical models' interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa632d46",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# AI's Aspiration and the Yin-Yang of Data Science\n",
    "\n",
    "- **AI's Ambition**: To mimic the human brain's structure in model complexity.\n",
    "\n",
    "- **Hinton's Argument**: Large number of parameters is necessary in AI models.\n",
    "\n",
    "- **Dualism in Data Science**: ML and statistics as complementary elements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bfd702",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# AI's Aspiration and the Yin-Yang of Data Science\n",
    "\n",
    "![](https://raw.githubusercontent.com/rauls3/R-Projects/main/1.1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c41ff9b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Exploration Patterns: Yin and Yang in Science\n",
    "\n",
    "- **Maritime Exploration Analogy**: Eastern route before Columbus's western journey.\n",
    "\n",
    "- **Cosmology's Shift**: From visible matter to exploring dark matter and dark energy.\n",
    "\n",
    "- **Data Science Evolution**: Starting with small parameter models in statistics, moving to ML's larger parameter models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbfb5b9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Breaking Traditional Constraints in ML\n",
    "\n",
    "- **The Parameter 'Sound Barrier'**: Overcoming the old constraint of parameter count.\n",
    "\n",
    "- **Real-World Applications**: ML excels in complex problems like image recognition, self-driving cars.\n",
    "\n",
    "- **The Yin and Yang of Data Size**: ML's rapid growth due to its capacity to handle large parameter spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144ff205",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Predictor Variables in Statistics vs. ML\n",
    "\n",
    "- **Predictor Selection**: A common practice in statistics, less so in ML.\n",
    "\n",
    "- **ML's Approach to Information**: Prefers to retain rather than discard data.\n",
    "\n",
    "- **Issues with Predictor Selection**: Risk of overestimating prediction skill."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c370c68",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The Tradeoff: Interpretability vs. Accuracy\n",
    "\n",
    "- **Statistics**: Offers interpretability with fewer parameters and predictors.\n",
    "\n",
    "- **Machine Learning**: Prioritizes accuracy over interpretability, especially in complex datasets.\n",
    "\n",
    "- **Evolving Complexity**: As data grows in size and complexity, ML's accuracy becomes more vital."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfaa7fd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Physics and Data Science: Parallel Evolutions\n",
    "\n",
    "- **Classical vs. Quantum Mechanics**: Transition from deterministic to probabilistic understanding.\n",
    "\n",
    "- **Modern Physics Approach**: Utilizing both classical and quantum mechanics as needed.\n",
    "\n",
    "- **Data Science's Duality**: Learning both statistics and ML for diverse data challenges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf094c2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction to Environmental Data Science"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c7d4e4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction to Environmental Data Science\n",
    "\n",
    "- **Definition**: Intersection of environmental science (ES) and data science.\n",
    "\n",
    "- **Branches of ES**: Includes atmospheric science, hydrology, oceanography, and more.\n",
    "\n",
    "- **Data Characteristics**: Each ES branch has unique data characteristics.\n",
    "\n",
    "- **Role of Statistics**: Long history of statistical methods application in various ES fields."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fac4e8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Characteristics of Environmental Data\n",
    "\n",
    "- **Data Types**: Environmental datasets usually contain continuous data (e.g., temperature, wind speed).\n",
    "\n",
    "- **Comparison with Non-Environmental Data**: Non-environmental ML datasets often have discrete or categorical data.\n",
    "\n",
    "- **Bounded vs. Unbounded Data**: Non-environmental data is typically bounded, while environmental data often isn't."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42e0189",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Predictive Modeling in Environmental Data Science\n",
    "\n",
    "- **Problem Types**: Classification for discrete output, regression for continuous output.\n",
    "\n",
    "- **Method Development**: Many ML methods initially designed for classification, later adapted for regression.\n",
    "\n",
    "- **Performance Evaluation**: Models are tested with separate datasets to evaluate accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f51749b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Challenges in Predictive Modeling with Environmental Data\n",
    "\n",
    "- **Outlier Issues**: Greater risk with unbounded continuous data than with bounded data.\n",
    "\n",
    "- **Extrapolation Risks**: Models trained on limited domains may yield inaccurate predictions for new, outlying data.\n",
    "\n",
    "- **Complexity in Environmental Predictions**: Accurate prediction using environmental data can be more challenging than in other domains."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c3194c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Outliers in Environmental Data Science\n",
    "\n",
    "![Outliers in Data Science](https://raw.githubusercontent.com/rauls3/R-Projects/img/Screenshot%202023-10-11%20at%2010.06.39.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f43bc0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Outliers in Environmental Data Science\n",
    "\n",
    "- **Figure Explanation**: Shows the challenge of outliers in 2-D input data.\n",
    "\n",
    "- **Training vs. Test Data**: Difference in ranges can lead to extrapolation errors.\n",
    "\n",
    "- **Case Study**: Excessive extrapolation in predicting air quality due to outlier test data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9455f51c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Air Quality Prediction Example\n",
    "\n",
    "- **Predictor Variable**: Cumulated precipitation for predicting \n",
    " concentration.\n",
    "\n",
    "- **Study Details**: Nonlinear regression model trained on 2013-2015 data, tested on 2010-2012 data.\n",
    "\n",
    "- **Extrapolation Issue**: Significant difference in precipitation levels between training and test datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36d4443",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Weather vs. Climate in Environmental Data Science\n",
    "\n",
    "- **Old Saying**: \"Climate is what you expect; weather is what you get.\"\n",
    "\n",
    "- **Data Grouping**: Short-term variations ('weather') vs long-term averages ('climate').\n",
    "\n",
    "- **Practical Implications**: Importance of seasonal forecasts in various industries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd572f5e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Central Limit Theorem's Effect on Data Nature\n",
    "\n",
    "- **Data Transformation**: Weather data averaged over time becomes climate data.\n",
    "\n",
    "- **Central Limit Theorem**: Causes weakening of non-linear relations in averaged data.\n",
    "\n",
    "- **Model Performance**: Non-linear models more effective for weather data than climate data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e97acf0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Central Limit Theorem's Effect on Data Nature\n",
    "\n",
    "![](https://raw.githubusercontent.com/rauls3/R-Projects/img/Screenshot%202023-10-11%20at%2010.06.52.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d054caa4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Climate Extremes in Data Analysis\n",
    "\n",
    "- **Shift in Focus**: Growing interest in climate extremes due to climate change.\n",
    "\n",
    "- **Climate Extremes Variables**: Derived from daily data, e.g., annual number of frost days.\n",
    "\n",
    "- **ML in Climate Extremes Study**: Increasing application in analyzing these variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df243619",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ML Adoption in Environmental Sciences\n",
    "\n",
    "- **Varied Germination Rates**: Dependence on existing models in respective fields.\n",
    "\n",
    "- **Meteorology vs. Hydrology**: Slower ML adoption in meteorology due to established numerical models.\n",
    "\n",
    "- **Hydrology and Remote Sensing**: Quicker acceptance of ML models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c1f572",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Challenges for ML Models in Environmental Sciences\n",
    "\n",
    "- **Need for Large Sample Sizes**: Nonlinear ML models require more data to outperform.\n",
    "\n",
    "- **Oceanography and Climate Science**: Slower ML adoption due to data collection difficulties and long timescales.\n",
    "\n",
    "- **Traditional vs. ML Approaches**: Initially separate, now merging within environmental sciences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44387148",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Recent Trends and Reviews in Environmental AI/ML\n",
    "\n",
    "- **Recent Developments**: Rapid ML growth even in fields like oceanography and climate science.\n",
    "\n",
    "- **Integration of ML and Physics**: Emerging trend of combining divergent approaches.\n",
    "\n",
    "- **Key Reviews**: Important literature on AI/ML's role in environmental sciences by Haupt, Gagne, and Hsieh."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8ad7a4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction to Curve Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90c4b9b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction to Curve Fitting\n",
    "\n",
    "- **Concept**: Curve fitting with one independent variable and one dependent variable \\( y \\).\n",
    "\n",
    "- **True Signal**: Quadratic relation \n",
    "\n",
    "$$ Y_{signal} = x - 0.25x^2 $$\n",
    "\n",
    "- **Data Composition**:\n",
    "\n",
    "$$Y = Y_{signal} + \\epsilon $$ \n",
    "\n",
    "where $\\epsilon$ is Gaussian noise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e011c635",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Synthetic Data Advantage\n",
    "\n",
    "- **Purpose**: Using synthetic data allows for a clear understanding of the true signal.\n",
    "\n",
    "- **Noise Characteristics**: Gaussian distribution with zero mean, standard deviation half of $Y_{signal}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e99888a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Polynomial Curve Fitting\n",
    "\n",
    "- **Polynomial Function**: $$\\hat{y} = w_0 + w_1x + w_2x^2 + ... + w_mx^m$$\n",
    "\n",
    "- **Adjustable Parameters**: $w_j$ are the weights for the polynomial, with $m + 1$ total parameters.\n",
    "\n",
    "- **Polynomial Orders**: Example uses orders 1, 2, 4, and 9 for fitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9279bb6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Minimizing Mean Squared Error (MSE)\n",
    "\n",
    "- **Objective**: Fit polynomials to data by minimizing the MSE between $\\hat{y}$ and data $y$.\n",
    "\n",
    "- **MSE Formula**: \n",
    "\n",
    "$$MSE = \\frac{1}{N} \\sum_{i=1}^N (\\hat{y_i} - y_i)^2 $$\n",
    "\n",
    "- **Data Points**: Fitting done with 11 data points in the example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a04311",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](https://raw.githubusercontent.com/rauls3/R-Projects/img/Screenshot%202023-10-11%20at%2010.07.46.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbbac4b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Underfitting and Overfitting Concepts\n",
    "\n",
    "- **Simple Linear Regression**: Order 1 polynomial is a straight line fit.\n",
    "\n",
    "- **Fit Improvement**: Better fit from order 1 to 2, but worsens at higher orders.\n",
    "\n",
    "- **Overfitting Example**: Order 9 polynomial fits training data but misses the true signal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4faa31",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Model Validation Importance\n",
    "\n",
    "- **Model Limitation**: Real-world scenarios don't reveal the true signal.\n",
    "\n",
    "- **Independent Validation Data**: Essential to detect overfitting or underfitting.\n",
    "\n",
    "- **MSE Trends**: Order 2 polynomial minimizes MSE for both training and validation data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5e4903",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Model Validation Importance\n",
    "\n",
    "![](https://raw.githubusercontent.com/rauls3/R-Projects/img/Screenshot%202023-10-11%20at%2010.07.57.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577f1edd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Impact of Training Data Size\n",
    "\n",
    "- **More Data Reduces Overfitting**: Comparison between 15 and 100 data points.\n",
    "\n",
    "- **Overfitting Reduction**: Order 9 polynomial fit improves with more data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0966f80",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Effect of Noise Level and Data Quantity\n",
    "\n",
    "- **High Noise Scenario**: Increased noise leads to worse overfitting.\n",
    "\n",
    "- **Large Data with High Noise**: Overfitting is reduced even with noisy data if the dataset is large.\n",
    "\n",
    "- **Modern Data Science Insight**: Ability to retrieve weak signals from noisy data with ample data points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b803f536",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](https://raw.githubusercontent.com/rauls3/R-Projects/img/Screenshot%202023-10-11%20at%2010.38.06.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc98ff5e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Extrapolation in Polynomial Solutions\n",
    "\n",
    "- **Extrapolation Domain**: Analysis beyond the training domain of \\( x \\in [-2, 2] \\).\n",
    "\n",
    "- **Solution Behavior**: Higher order polynomials perform poorly outside the training domain.\n",
    "\n",
    "- **Reproducibility Issues**: Different extrapolations with different random data initializations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d40c36",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](https://raw.githubusercontent.com/rauls3/R-Projects/img/Screenshot%202023-10-11%20at%2010.38.20.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a95715",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Challenges with Polynomial Extrapolation\n",
    "\n",
    "- **Rapid Increase Outside Domain**: High order polynomials grow quickly as $x \\rightarrow \\pm \\infty$\n",
    "\n",
    "- **Modern Data Science Methods**: Use of artificial neural networks with less aggressive growth basis functions.\n",
    "\n",
    "- **Extrapolation Taming**: Efforts to reduce wild extrapolation, though not entirely eliminated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99d07e1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Basic Types of Data in Data Science and Types of Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062f4337",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Basic Types of Data in Data Science\n",
    "\n",
    "## A. Discrete or Categorical Variables\n",
    "- **Nature**: Defined, distinct categories or specific values.\n",
    "- **Examples**: Binary (e.g., [0, 1]), States (e.g., [on, off]), Truth Values (e.g., [true, false]).\n",
    "- **Environmental Science Applications**: Weather states ([storm, no storm]), Temperature ranges ([cold, normal, warm]).\n",
    "\n",
    "## B. Continuous Variables\n",
    "- **Nature**: Variables that can take any value within a range.\n",
    "- **Environmental Science Applications**: Temperature, wind speed, pollutant concentration.\n",
    "\n",
    "## C. Probability Distributions\n",
    "- **Role**: Describe the likelihood of different outcomes.\n",
    "- **Examples**: Gaussian distribution for temperature, Weibull distribution for wind speed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aacfa00",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Data Descriptions and Preferences in Machine Learning and Statistics\n",
    "\n",
    "## Machine Learning vs. Statistics\n",
    "- **Early ML Focus**: Predominantly on discrete/categorical data, especially in commercial/engineering fields.\n",
    "- **Environmental Science Preference**: Continuous data for specific predictions (e.g., exact temperature).\n",
    "- **Statistical Approach**: Use of probability distributions for detailed predictions (e.g., temperature with mean and standard deviation).\n",
    "\n",
    "## Linkage Between ML and Statistics\n",
    "- **Historical Gap**: Initial lack of strong connection between ML and statistical methods.\n",
    "- **Recent Developments**: Improved integration, with ML methods increasingly cast in probabilistic frameworks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d19f6d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Types of Learning in Data Science\n",
    "\n",
    "## Supervised Learning\n",
    "- **Analogy**: Learning with a teacher's guidance (in this case, an objective).\n",
    "\n",
    "## Unsupervised Learning\n",
    "- **Analogy**: Solitary learning (e.g., child solving a jigsaw puzzle on their own).\n",
    "- **Nature**: Relying on self-organization without direct teaching.\n",
    "\n",
    "## Reinforced Learning\n",
    "- **Brief Description**: A less common form of learning, involving learning from feedback."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b0115c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3fa20d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction to Supervised Learning\n",
    "\n",
    "## Definition and Process\n",
    "- **Objective**: To find a mapping from input variables $X$ to output variables $\\hat{y}$.\n",
    "- **Input Variables**: Also known as predictors, features, attributes, or covariates.\n",
    "- **Output Variables**: Also known as response variables or predictands.\n",
    "- **Notation**: $\\{x_i, y_i\\}$ represents training data, where $i = 1, ..., N$ and $N$ is the sample size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0ea6ee",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Types of Supervised Learning\n",
    "\n",
    "## Regression\n",
    "- **Description**: Output variables are real or continuous (e.g., predicting next day's wind speed from temperature, humidity, and pressure).\n",
    "- **Input Nature**: Usually real variables, but can include discrete/categorical variables.\n",
    "\n",
    "## Classification\n",
    "- **Description**: Output variables are discrete/categorical (e.g., classifying weather as ‘storm’ or ‘no storm’).\n",
    "- **Binary vs. Multi-Class**: Binary classification for two classes, multi-class for more than two (e.g., classifying seasonal temperatures or satellite images)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94b98b4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Supervised Learning in Environmental Science vs. Other Fields\n",
    "\n",
    "## Environmental Science Applications\n",
    "- **Common Use**: Tends to focus more on regression problems.\n",
    "- **Combined Approach**: Some problems use both classification and regression (e.g., precipitation forecast).\n",
    "\n",
    "## Non-Environmental Applications\n",
    "- **Common Use**: Predominantly classification (e.g., spam filters, credit card fraud detection, handwriting recognition).\n",
    "- **Large Class Variety**: Especially in applications like object recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac2cce7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24dfde87",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction to Unsupervised Learning\n",
    "\n",
    "## Concept and Goal\n",
    "- **Key Difference from Supervised Learning**: No output data $\\hat{y}$, only input data $x$.\n",
    "- **Objective**: To find hidden structure or patterns within the input data $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31475724",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Applications of Unsupervised Learning\n",
    "\n",
    "## Clustering\n",
    "- **Purpose**: Grouping similar data points in the input space.\n",
    "- **Example**: Identifying teleconnection patterns in atmospheric data.\n",
    "\n",
    "## Dimension Reduction\n",
    "- **Purpose**: Condensing high-dimensional data into lower dimensions.\n",
    "- **Techniques**: Such as principal component analysis.\n",
    "- **Example**: Reducing environmental datasets from 100 dimensions to 2 or 3 dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98355922",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The Significance of Unsupervised Learning\n",
    "\n",
    "## Perspectives on Unsupervised Learning\n",
    "- **Geoffrey Hinton's View**: Emphasizes the vastness of unsupervised learning in human learning.\n",
    "- **Quote**: Highlights the inefficiency of learning one bit per second compared to the need for 10 bits per second from input.\n",
    "\n",
    "## Deep Learning and Unsupervised Learning\n",
    "- **Review by LeCun, Bengio, et al.**: Initially catalytic, then overshadowed by supervised learning.\n",
    "- **Future Expectation**: Anticipation of unsupervised learning gaining more importance, mirroring human and animal learning patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74ae749",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Understanding the Curse of Dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99888223",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Understanding the Curse of Dimensionality\n",
    "\n",
    "## Definition\n",
    "\n",
    "- **Origin**: Coined by Bellman in 1961.\n",
    "- **Problem**: Data methods for low-dimensional datasets become ineffective in high dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce87e048",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Dimensionality and Data Coverage\n",
    "\n",
    "## Illustrative Example\n",
    "- **1-D Space**: A segment of width 0.5 covers half of the unit interval [0, 1].\n",
    "- **2-D Space**: A square of width 0.5 covers a quarter of the unit square.\n",
    "- **3-D and Beyond**: Exponential decrease in coverage with increasing dimensions.\n",
    "\n",
    "![](https://raw.githubusercontent.com/rauls3/R-Projects/img/Screenshot%202023-10-11%20at%2010.08.11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc155a8a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Impact on Data Sampling and Techniques\n",
    "\n",
    "## Sampling Challenges\n",
    "\n",
    "- **High-Dimensional Spaces**: Sparse data points in higher dimensions, e.g., less than 0.1 data point per hypercube in 10-D space.\n",
    "- **Implication**: Difficulty in obtaining representative samples in high-dimensional spaces.\n",
    "\n",
    "## Technique Breakdown\n",
    "\n",
    "- **Example**: K-nearest neighbors become ineffective as neighbors are too far in high dimensions.\n",
    "- **Polynomial Fit Challenges**: Poor generalization to high dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73569a63",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Working with High-Dimensional Data\n",
    "\n",
    "## Strategies for High Dimensions\n",
    "\n",
    "- **Data Concentration**: Real high-dimensional data often lie in lower effective dimensions.\n",
    "- **Exploiting Smoothness**: Using properties like local interpolation in real data.\n",
    "\n",
    "## Successful Methods\n",
    "\n",
    "- **Approach**: Methods that leverage these properties tend to work well in high-dimensional spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de13198f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Questions?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d951adc4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# See you in the next class!"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
